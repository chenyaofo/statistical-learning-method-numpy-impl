{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "naive_bayes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPsi4jdmVFYWvGqg/4d5C7A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenyaofo/statistical-learning-method-numpy-impl/blob/main/algorithms/naive_bayes_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "9IswGZYpXrnb"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can choose a specific datasets from libsvm (https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/)\n",
        "# here we choose dataset australian (https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#australian)\n",
        "# set the download_link from the website (note that here we download the scaled version)\n",
        "download_link = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/australian_scale\"\n",
        "# set the #features according to the info in the website\n",
        "n_features = 14\n",
        "# set the test set proportion to 50%\n",
        "test_size = 0.5"
      ],
      "metadata": {
        "id": "A4G9lXyHX5I-"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_process_data(download_link, n_features, test_size):\n",
        "    # download the dataset in a online way\n",
        "    r = requests.get(download_link)\n",
        "\n",
        "    # load the dataset by sklearn api\n",
        "    X, y = load_svmlight_file(BytesIO(r.content), n_features=n_features)\n",
        "    X = X.toarray()\n",
        "\n",
        "    n_samples, n_features = X.shape\n",
        "    X = numpy.column_stack((X, numpy.ones((n_samples, 1))))\n",
        "    y = y.reshape((-1, 1))\n",
        "\n",
        "    # the default type of label is float, we cast it into int32\n",
        "    y = y.astype(numpy.int32)\n",
        "\n",
        "    # we transform label -1 to 0 since the function 'bincount' only receives non-negative inputs\n",
        "    y[y==-1]=0\n",
        "\n",
        "    print(max(y))\n",
        "    print(min(y))\n",
        "\n",
        "    if test_size is not None:\n",
        "        # split the dataset into train and val sets by sklearn api\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5)\n",
        "        return X_train, X_val, y_train, y_val\n",
        "    else:\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "QZ-iDu4VX9rM"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the shape of X_train is (#train_set, n_features)\n",
        "# the shape of X_val is (#val_set, n_features)\n",
        "# the shape of y_train is (#train_set, 1)\n",
        "# the shape of y_val is (#val_set, 1)\n",
        "# in this dataset, each sample has 14-dimension features, each dimension is a real number, the label belong to {0,1}\n",
        "X_train, X_val, y_train, y_val = download_process_data(download_link, n_features, test_size)\n",
        "\n",
        "# we find the maximum label in train and val sets, which is a prameter for the function 'bincount'\n",
        "maximum_label = numpy.max(numpy.concatenate([y_train, y_val], axis=0))\n",
        "\n",
        "X_train = X_train[:, :13]\n",
        "X_val = X_val[:, :13]\n",
        "n_features = 13"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNmqiXJ-YDjd",
        "outputId": "651c13ff-efee-4f14-8aac-90077dce3443"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n",
            "[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# P(label | features) = P(features | label) * P(label) / P(features)"
      ],
      "metadata": {
        "id": "N_YKaVCBYD2U"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: calculate P(features | label)\n",
        "\n",
        "# sicne P(features) is a joint distibution, we calculate it based on the assumption that\n",
        "# each feature is independent and identically distributed\n",
        "# here we assume each feature follows a Gaussian distribution\n",
        "# thus we calcuate the mean and std of Gaussian distribution\n",
        "\n",
        "def gaussian_distribution_pdf(x, mean, std):\n",
        "    return (1 / (numpy.sqrt(2 * numpy.pi) * std+1e-8)) * numpy.exp(-(x-mean)**2/(2*std**2+1e-8))\n",
        "# y_train[-1,-1]=2\n",
        "a = numpy.hstack((X_train,y_train))\n",
        "a = a[a[:, -1].argsort()]\n",
        "a = numpy.split(a[:,:-1], numpy.unique(a[:, -1], return_index=True)[1][1:])\n",
        "# print(a[2].shape)\n",
        "for g in a:\n",
        "    print(g.shape)\n",
        "print(max(y_train))\n",
        "print(min(y_train))\n",
        "means = []\n",
        "stds = []\n",
        "\n",
        "for group in a:\n",
        "    mean = numpy.mean(group, axis=0, keepdims=True)\n",
        "    std = numpy.std(group, axis=0, keepdims=True)\n",
        "    means.append(mean)\n",
        "    stds.append(std)\n",
        "\n",
        "means = numpy.concatenate(means, axis=0) # (max_label, n_features)\n",
        "stds = numpy.concatenate(stds, axis=0) # (max_label, n_features)\n",
        "\n",
        "print(means.shape)\n",
        "\n",
        "p_features_label = gaussian_distribution_pdf(X_val, numpy.expand_dims(means, 1), numpy.expand_dims(stds, 1)) #(#val_set, n_features) (max_label, n_features) (max_label, n_features)\n",
        "print(p_features_label.shape) # (max_label, #val_set, n_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PfPlOk7dqPI",
        "outputId": "77864888-1857-4d83-f0ab-5005da5abf43"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(180, 13)\n",
            "(165, 13)\n",
            "[1]\n",
            "[0]\n",
            "(2, 13)\n",
            "(2, 345, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 2: calculate P(label)\n",
        "\n",
        "# the marginal distribution of label y can be directly calculated:\n",
        "label_freq = numpy.bincount(y_train.reshape(-1), minlength=maximum_label+1)\n",
        "p_label = label_freq / label_freq.sum() # (max_label,)"
      ],
      "metadata": {
        "id": "Fj2MBldsdvuX"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(p_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhxJY1A0d4DG",
        "outputId": "7f6d5d25-a769-4273-e0dd-568c7d75a093"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.52173913 0.47826087]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 3: calculate P(features)\n",
        "global_mean = numpy.mean(X_train, axis=0, keepdims=True)\n",
        "global_std = numpy.std(X_train, axis=0, keepdims=True)\n",
        "\n",
        "p_features = gaussian_distribution_pdf(X_val, global_mean, global_std)\n",
        "print(p_features.shape) # (#val_set, n_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gctSKz4oeKDV",
        "outputId": "0cc2af9d-0941-44c8-b08a-74a8c5b5ec84"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(345, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(y_gt:numpy.array, y_predict:numpy.array):\n",
        "    # number of correct samples / number of total samples\n",
        "    return (y_gt == y_predict).sum() / y_gt.size"
      ],
      "metadata": {
        "id": "EMrxA_C9GxI3"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_baive_bayes():\n",
        "    pass\n",
        "\n",
        "predict_score = numpy.log(p_features_label).sum(axis=-1) * numpy.expand_dims(p_label,axis=1) / numpy.log(p_features).sum(axis=-1)"
      ],
      "metadata": {
        "id": "J7tALBGbZlxb"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_score.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP8O9tJkdeza",
        "outputId": "842b60cf-2b3a-4303-a195-4409a53120a5"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 345)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = predict_score.argmin(0)"
      ],
      "metadata": {
        "id": "E_QV8asAdn9Y"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_predict.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUsQYvl2dtd4",
        "outputId": "b0f3d4eb-751f-48e1-e437-38eaf24b4805"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(345,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = numpy.expand_dims(y_predict,axis=1)"
      ],
      "metadata": {
        "id": "_NbHSJxadvyo"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the accuracy on predicted samples of the val set\n",
        "print(f\"The accuracy on val set is {compute_accuracy(y_val, y_predict)*100.0:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g70Mar7-d2eP",
        "outputId": "31f6d183-afb4-4059-bf8f-3ed881c25722"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy on val set is 87.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "278yr5QNd29f"
      },
      "execution_count": 136,
      "outputs": []
    }
  ]
}