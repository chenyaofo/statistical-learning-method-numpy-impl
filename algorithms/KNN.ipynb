{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWxxUFmb/NVUl51/lTaiZ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenyaofo/statistical-learning-method-numpy-impl/blob/main/algorithms/KNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y_zuRek-PSXk"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can modify the following 'download_link' and `n_features` to switch to another dataset\n",
        "# !!! note that this impl can not be used for a large dataset since it predict the total val samples\n",
        "# in a single batch, this requires great memory footprint\n",
        "\n",
        "# we can choose a specific datasets from libsvm (https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/)\n",
        "# here we choose dataset australian (https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#australian)\n",
        "# set the download_link from the website (note that here we download the scaled version)\n",
        "download_link = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/australian_scale\"\n",
        "# set the #features according to the info in the website\n",
        "n_features = 14\n",
        "# set the test set proportion to 50%\n",
        "test_size = 0.5"
      ],
      "metadata": {
        "id": "NkYXofFPSD8V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_process_data(download_link, n_features, test_size):\n",
        "    # download the dataset in a online way\n",
        "    r = requests.get(download_link)\n",
        "\n",
        "    # load the dataset by sklearn api\n",
        "    X, y = load_svmlight_file(BytesIO(r.content), n_features=n_features)\n",
        "    X = X.toarray()\n",
        "\n",
        "    n_samples, n_features = X.shape\n",
        "    X = numpy.column_stack((X, numpy.ones((n_samples, 1))))\n",
        "    y = y.reshape((-1, 1))\n",
        "\n",
        "    if test_size is not None:\n",
        "        # split the dataset into train and val sets by sklearn api\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5)\n",
        "        return X_train, X_val, y_train, y_val\n",
        "    else:\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "torGpm1HSB_A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the shape of X_train is (#train_set, n_features)\n",
        "# the shape of X_val is (#val_set, n_features)\n",
        "# the shape of y_train is (#train_set, 1)\n",
        "# the shape of y_val is (#val_set, 1)\n",
        "X_train, X_val, y_train, y_val = download_process_data(download_link, n_features, test_size)\n",
        "# the default type of label is float, we cast it into int32\n",
        "y_train, y_val = y_train.astype(numpy.int32), y_val.astype(numpy.int32)\n",
        "\n",
        "# we transform label -1 to 0 since the function 'bincount' only receives non-negative inputs\n",
        "y_train[y_train==-1]=0\n",
        "y_val[y_val==-1]=0\n",
        "\n",
        "# we find the maximum label in train and val sets, which is a prameter for the function 'bincount'\n",
        "maximum_label = numpy.max(numpy.concatenate([y_train, y_val], axis=0))"
      ],
      "metadata": {
        "id": "LWygfiLUTDCm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we define euclidean metric, other distance metric is also ok\n",
        "def compute_euclidean_distance(X_train:numpy.array, X_val:numpy.array):\n",
        "    # we expand some dimenstions for these two matrixs, in this case,\n",
        "    # we can compute the distance between every two samples by broadcast \n",
        "    X_train = numpy.expand_dims(X_train, axis=0)\n",
        "    X_val = numpy.expand_dims(X_val, axis=1)\n",
        "    return numpy.sqrt(((X_val - X_train)**2).sum(axis=-1)) # return shape is (#val_set, #train_set)"
      ],
      "metadata": {
        "id": "D08CIv2wSYCs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we define a function to find topk nearest samples (called closest set) of a given val/test sample x\n",
        "# note that we impl it in a batch way, which is more efficient\n",
        "def find_closest_set(distances:numpy.array, X_train:numpy.array, y_train:numpy.array, topk:int):\n",
        "    ind = numpy.argpartition(distances, kth=topk, axis=-1)[:,:topk]\n",
        "    return numpy.take(X_train, ind, axis=0), numpy.take(y_train, ind, axis=0)"
      ],
      "metadata": {
        "id": "mq38iI41SZZc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we define a function to predict the label of a given val/test sample x based on its closest set\n",
        "def predict_in_closest_set(y_closest_set:numpy.array, maximum_label:int):\n",
        "    # we predict the label from a given sample by vote\n",
        "    # we adopt the label with highest frequency in the closest set\n",
        "    freq = numpy.apply_along_axis(numpy.bincount, 1, y_closest_set, minlength=maximum_label+1)\n",
        "    return freq.argmax(axis=1)"
      ],
      "metadata": {
        "id": "jEqg9hOVow_Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_nearest_algo(X_train:numpy.array, y_train:numpy.array,\n",
        "                   X_val:numpy.array, topk:int, maximum_label:int):\n",
        "    # step 1: compute the distance between every two samples in train ans val set\n",
        "    distances = compute_euclidean_distance(X_train, X_val) # (#val_set, #train_set)\n",
        "    # step 2: for each val/test sample, we find its topk nearest train samples\n",
        "    x_closest_set, y_closest_set = find_closest_set(distances, X_train, y_train, topk) # (#val_set, topk, n_features), (#val_set, topk, 1)\n",
        "    # step 3: we vote for final prediction based on above found nearest train samples\n",
        "    if topk == 1: # when k=1, we do not need to vote\n",
        "        return y_closest_set.squeeze(axis=1) # (#val_set, 1)\n",
        "    else:\n",
        "        return predict_in_closest_set(y_closest_set, maximum_label) # (#val_set, 1)"
      ],
      "metadata": {
        "id": "RWdzkEXYUAqh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "# here we set some hyper-parameters\n",
        "topk = 5 # we set K to 5 in the algorithm\n",
        "# run k nearest algo\n",
        "y_predict = k_nearest_algo(X_train, y_train, X_val, topk, maximum_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cRGnBXTr7A3",
        "outputId": "45c2825a-9dc7-488b-fa5f-7e5387920111"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 7.63 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(y_gt:numpy.array, y_predict:numpy.array):\n",
        "    # number of correct samples / number of total samples\n",
        "    return (y_gt == y_predict).sum() / y_gt.size"
      ],
      "metadata": {
        "id": "T3tvzcRGSldj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the accuracy on predicted samples of the val set\n",
        "print(f\"The accuracy on val set is {compute_accuracy(y_val, y_predict)*100.0:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pENnEtpZsKgQ",
        "outputId": "7bce42fe-c6cb-47a5-8c9b-1bbaee14b6a7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy on val set is 84.93%\n"
          ]
        }
      ]
    }
  ]
}